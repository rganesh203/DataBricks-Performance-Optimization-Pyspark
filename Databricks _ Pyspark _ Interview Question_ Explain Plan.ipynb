{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672f8d59-2160-4857-91b4-67b99dce486c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Understanding and using the explain plan in Databricks and PySpark is crucial for optimizing performance and troubleshooting issues in Spark applications. The explain method provides insights into how Spark plans to execute a query, revealing the physical and logical plans, which can help in identifying potential bottlenecks and inefficiencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2860545e-c939-483c-a98e-420db0e25b63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###What is the Explain Plan?\n",
    "The explain plan in PySpark shows the execution plan of a DataFrame query. This plan includes details about:\n",
    "\n",
    "Logical Plan: The initial abstract representation of the query.\n",
    "Optimized Logical Plan: The logical plan after optimization rules have been applied.\n",
    "Physical Plan: The plan that shows how Spark will execute the query, including details about shuffles, exchanges, scans, joins, and other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4dccf4c-9b41-496d-90b1-6e9a6163acfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Using the Explain Plan\n",
    "To use the explain method in PySpark, you call it on a DataFrame. You can specify different levels of details by passing parameters to the explain method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83dc515-65f8-474f-82c8-a14fe0cba91c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Example Scenario\n",
    "Consider the following scenario where we have two DataFrames that we want to join and then perform some transformations. We will use the explain method to understand the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992ddab6-a5c4-4e68-a905-99167936b52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project [id#2254L, name#2255, department#2259, ((name#2255 + _) + department#2259) AS name_department#2265]\n+- Project [id#2254L, name#2255, department#2259]\n   +- Join Inner, (id#2254L = id#2258L)\n      :- RepartitionByExpression [id#2254L], 4\n      :  +- LogicalRDD [id#2254L, name#2255], false\n      +- RepartitionByExpression [id#2258L], 4\n         +- LogicalRDD [id#2258L, department#2259], false\n\n== Analyzed Logical Plan ==\nid: bigint, name: string, department: string, name_department: double\nProject [id#2254L, name#2255, department#2259, ((cast(name#2255 as double) + cast(_ as double)) + cast(department#2259 as double)) AS name_department#2265]\n+- Project [id#2254L, name#2255, department#2259]\n   +- Join Inner, (id#2254L = id#2258L)\n      :- RepartitionByExpression [id#2254L], 4\n      :  +- LogicalRDD [id#2254L, name#2255], false\n      +- RepartitionByExpression [id#2258L], 4\n         +- LogicalRDD [id#2258L, department#2259], false\n\n== Optimized Logical Plan ==\nProject [id#2254L, name#2255, department#2259, null AS name_department#2265]\n+- Join Inner, (id#2254L = id#2258L)\n   :- RepartitionByExpression [id#2254L], 4\n   :  +- Filter isnotnull(id#2254L)\n   :     +- LogicalRDD [id#2254L, name#2255], false\n   +- RepartitionByExpression [id#2258L], 4\n      +- Filter isnotnull(id#2258L)\n         +- LogicalRDD [id#2258L, department#2259], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#2254L, name#2255, department#2259, null AS name_department#2265]\n   +- SortMergeJoin [id#2254L], [id#2258L], Inner\n      :- Sort [id#2254L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id#2254L, 200), REPARTITION_BY_NUM, [plan_id=1728]\n      :     +- Filter isnotnull(id#2254L)\n      :        +- Scan ExistingRDD[id#2254L,name#2255]\n      +- Sort [id#2258L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(id#2258L, 200), REPARTITION_BY_NUM, [plan_id=1729]\n            +- Filter isnotnull(id#2258L)\n               +- Scan ExistingRDD[id#2258L,department#2259]\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explain Plan Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create example DataFrames\n",
    "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")]\n",
    "data2 = [(1, \"HR\"), (2, \"Engineering\"), (3, \"Finance\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"department\"])\n",
    "\n",
    "# Repartition to simulate larger datasets\n",
    "df1 = df1.repartition(4, \"id\")\n",
    "df2 = df2.repartition(4, \"id\")\n",
    "\n",
    "# Perform a join operation\n",
    "joined_df = df1.join(df2, \"id\")\n",
    "\n",
    "# Add a transformation\n",
    "result_df = joined_df.withColumn(\"name_department\", \n",
    "                                 joined_df[\"name\"] + \"_\" + joined_df[\"department\"])\n",
    "\n",
    "# Show explain plan\n",
    "result_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "489994c9-3829-4c18-960f-47eaad7b0eae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Explanation of the Explain Plan\n",
    "\n",
    "When you run result_df.explain(True), you get a detailed execution plan that typically includes:\n",
    "\n",
    "Logical Plan:\n",
    "\n",
    "The logical representation of the query before any optimizations.\n",
    "Shows the structure of the DataFrame operations and transformations.\n",
    "\n",
    "Optimized Logical Plan:\n",
    "\n",
    "The logical plan after Sparkâ€™s Catalyst optimizer has applied various optimization rules.\n",
    "Includes operations like predicate pushdown, projection pruning, and other logical optimizations.\n",
    "\n",
    "Physical Plan:\n",
    "\n",
    "The actual execution steps Spark will take to run the query.\n",
    "Includes details on stages, tasks, shuffles, joins, and other physical operations.\n",
    "Provides insight into how data will be moved and transformed across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2929e7d7-20cf-460e-9113-367b3a165a97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "== Physical Plan ==\n",
    "*(5) SortMergeJoin [id#0], [id#2], Inner\n",
    ":- *(2) Sort [id#0 ASC NULLS FIRST], false, 0\n",
    ":  +- Exchange hashpartitioning(id#0, 4), REPARTITION_BY_NUM\n",
    ":     +- *(1) Project [id#0, name#1]\n",
    ":        +- *(1) Filter isnotnull(id#0)\n",
    ":           +- *(1) Scan ExistingRDD[id#0,name#1]\n",
    "+- *(4) Sort [id#2 ASC NULLS FIRST], false, 0\n",
    "   +- Exchange hashpartitioning(id#2, 4), REPARTITION_BY_NUM\n",
    "      +- *(3) Project [id#2, department#3]\n",
    "         +- *(3) Filter isnotnull(id#2)\n",
    "            +- *(3) Scan ExistingRDD[id#2,department#3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e4b8369-ebd6-4bc1-bf4a-36f201c3e4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Key Components of the Physical Plan\n",
    "\n",
    "###SortMergeJoin:\n",
    "Indicates that Spark is using a Sort-Merge Join for this operation.\n",
    "Join keys: [id#0] from the first DataFrame and [id#2] from the second DataFrame.\n",
    "\n",
    "###Sort:\n",
    "Before merging, each dataset is sorted by the join key (id).\n",
    "\n",
    "###Exchange:\n",
    "Represents shuffling of data across the cluster nodes based on the join key (hashpartitioning).\n",
    "\n",
    "###Project:\n",
    "Shows the selection of specific columns.\n",
    "\n",
    "###Filter:\n",
    "Indicates any filtering applied to the data.\n",
    "\n",
    "###Scan:\n",
    "Represents reading the data from the original DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa25102-3991-497c-bd46-0daa6c033848",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Best Practices\n",
    "Understand Shuffles: Shuffling is expensive. Minimize unnecessary shuffles by properly partitioning your data.\n",
    "Use Caching: Cache intermediate DataFrames to avoid recomputation.\n",
    "Monitor Execution: Use the Spark UI to monitor the execution of your jobs and identify bottlenecks.\n",
    "Optimize Joins: Use broadcast joins for smaller tables and ensure proper partitioning for large joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b300d8b0-deb0-40d7-98d7-0c9406353a56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Conclusion\n",
    "Using explain() in PySpark within Databricks helps you understand the execution plan of your queries, allowing you to optimize and debug effectively. By interpreting the logical and physical plans, you can identify performance bottlenecks and make informed decisions to enhance the efficiency of your Spark jobs. This understanding is crucial for performance optimization and is a common topic in interviews."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks | Pyspark | Interview Question: Explain Plan",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
