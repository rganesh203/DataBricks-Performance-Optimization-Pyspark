{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a3b363-110d-4595-ae37-9dfe038f871d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In Databricks and Apache Spark, both cache() and persist() methods are used to optimize performance by storing intermediate DataFrame or RDD results in memory (or optionally on disk). Here’s an explanation of each, along with examples of when to use them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffb868df-31c9-4eb8-8c30-d4aa3e8b3352",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "cache()\n",
    "The cache() method is a shorthand for persist(StorageLevel.MEMORY_ONLY) in Spark. It marks the DataFrame or RDD to be cached in memory only. Here’s how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e2a5315-41cf-4f63-a114-bff0c8e1d5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example of caching a DataFrame\n",
    "df = spark.read.parquet(\"path/to/data\")\n",
    "df.cache()\n",
    "\n",
    "# Perform operations on cached DataFrame\n",
    "result = df.filter(df[\"column\"] > 100).groupBy(\"another_column\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771081d6-14c1-41e0-899c-3d77e9647307",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "persist()\n",
    "The persist() method allows you to specify different storage levels (MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.) based on your specific requirements. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b5d89d-54d1-4259-9cc0-5306449ae70e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example of persisting a DataFrame with specific storage level\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df = spark.read.parquet(\"path/to/data\")\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# Perform operations on persisted DataFrame\n",
    "result = df.filter(df[\"column\"] > 100).groupBy(\"another_column\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57db060b-ea23-4468-afca-543f835677c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When to Use cache() vs persist()\n",
    "Use cache():\n",
    "\n",
    "When you want a quick way to cache the DataFrame or RDD in memory without specifying the storage level explicitly.\n",
    "Useful for scenarios where Spark's default memory-only caching (MEMORY_ONLY) is sufficient and you don’t need to persist the data beyond the current Spark job.\n",
    "Use persist():\n",
    "\n",
    "When you need to customize the storage level (e.g., use MEMORY_AND_DISK for larger datasets that may not fit entirely in memory).\n",
    "Useful when you want the cached data to survive Spark job restarts or when you need to persist to disk due to memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f811fcd-621d-4557-823e-d5ad1de51bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example Scenario\n",
    "Let's consider a scenario where you have a large dataset and want to optimize performance by caching or persisting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be631fbe-67bc-490b-b2a2-1bc62c4be097",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a large dataset\n",
    "df = spark.read.csv(\"path/to/large_dataset.csv\")\n",
    "\n",
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform multiple operations on the cached DataFrame\n",
    "result1 = df.filter(df[\"_c0\"] == \"value1\").count()\n",
    "result2 = df.filter(df[\"_c1\"] == \"value2\").groupBy(\"_c2\").count().show()\n",
    "\n",
    "# Unpersist the DataFrame after use\n",
    "df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b4abc3-44ab-4dce-b901-5cc3c202ee70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this example:\n",
    "\n",
    "cache() is used to store the DataFrame in memory.\n",
    "Operations (filter(), groupBy()) are performed on the cached DataFrame, leveraging the cached data for faster computation.\n",
    "unpersist() is used to remove the DataFrame from cache once it's no longer needed, freeing up memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "146a1154-5315-4872-aa06-41e28eaf1bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tips for Optimization\n",
    "\n",
    "Memory Management: Monitor memory usage in Databricks and adjust caching or persistence accordingly to prevent out-of-memory errors.\n",
    "\n",
    "Storage Levels: Choose the appropriate storage level (MEMORY_ONLY, MEMORY_AND_DISK, etc.) based on your data size and processing requirements.\n",
    "\n",
    "Performance Testing: Benchmark both caching and persistence strategies to determine which works best for your specific workload.\n",
    "\n",
    "By using cache() and persist() effectively in Databricks and Apache Spark, you can optimize performance by reducing computation time and improving overall data processing efficiency."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Spark | Cache vs Persist",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
