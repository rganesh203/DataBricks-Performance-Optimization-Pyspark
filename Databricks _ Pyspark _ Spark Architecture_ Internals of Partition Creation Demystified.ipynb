{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda406da-158d-4aa1-b5cb-83b50230ea31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Understanding the internals of partition creation in Apache Spark, especially in the context of Databricks using PySpark, involves delving into how Spark manages data distribution across its compute resources. Here’s a breakdown of the key concepts and processes involved:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7775d6b1-5007-4afe-b172-237576e66c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark Architecture Overview\n",
    "Apache Spark processes data using a distributed computing model, where data is divided into partitions and distributed across a cluster of nodes for parallel processing. Each node (or executor) in the cluster operates on a subset of the data, and partitions are the basic unit of parallelism and distribution in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7cea1da-abb6-49e7-b848-2dacfc9ee745",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Internals of Partition Creation\n",
    "###Data Source Partitioning:\n",
    "\n",
    "Default Partitioning: When you load data into Spark from a source (like a file), Spark automatically determines the number of partitions based on the size and type of the data source. For example, when you read from a CSV file, Spark typically creates one partition per file block.\n",
    "Manual Partitioning: You can manually control partitioning when reading data using options like spark.read.option(\"numPartitions\", num_partitions) to specify the number of partitions or using partitioning columns with partitionBy() when writing data.\n",
    "\n",
    "###Partitioning Strategies:\n",
    "\n",
    "Range Partitioning: Spark can partition data based on ranges of a column's values, ensuring that rows with similar values are processed together.\n",
    "Hash Partitioning: Data can be partitioned based on the hash value of a column, which evenly distributes data across partitions.\n",
    "Custom Partitioning: Users can define custom partitioning strategies by implementing the Partitioner interface in Spark.\n",
    "\n",
    "###Data Distribution and Execution:\n",
    "\n",
    "Once partitions are created, Spark distributes them across the cluster nodes. Each partition is processed independently by the tasks running on these nodes.\n",
    "Spark tries to keep partitions evenly sized to ensure workload balance across executors and minimize data shuffling during operations like joins and aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c34d4e-1c68-4d31-ace6-4f31c24b7e45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example Scenario in Databricks (PySpark)\n",
    "Here’s a simplified example demonstrating how partitioning works in practice using PySpark in Databricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4eb65e-f424-4cd9-afa1-1d65eda78628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3516885369338651>:13\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    .option(\"numPartitions\", 4) \\  # Specify number of partitions\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected character after line continuation character\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-3516885369338651>:13\u001B[0;36m\u001B[0m\n\u001B[0;31m    .option(\"numPartitions\", 4) \\  # Specify number of partitions\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected character after line continuation character\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: unexpected character after line continuation character (<command-3516885369338651>, line 13)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partitioning Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example data source and read with manual partitioning\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"numPartitions\", 4) \\  # Specify number of partitions\n",
    "    .load(\"dbfs:/FileStore/tables/data.csv\")\n",
    "\n",
    "# Show partition count\n",
    "print(\"Number of partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Example of range partitioning\n",
    "df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"category\") \\  # Partitioning by 'category' column\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dbfs:/FileStore/tables/partitioned_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3742e5ce-9bc8-4a15-a70f-2c218c7606c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Explanation:\n",
    "Data Loading: The spark.read.format(\"csv\") command loads data from a CSV file. Here, .option(\"numPartitions\", 4) specifies that Spark should create 4 partitions when reading the data.\n",
    "Data Writing: The .partitionBy(\"category\") command instructs Spark to write the data partitioned by the category column into Parquet format. This uses range partitioning based on the distinct values in the category column.\n",
    "\n",
    "###Key Points:\n",
    "Efficiency: Proper partitioning is crucial for optimizing Spark jobs by reducing data shuffling and improving parallelism.\n",
    "Data Skew: Uneven data distribution (skew) across partitions can impact performance, so monitoring and adjusting partitioning strategies as needed is important.\n",
    "Advanced Features: Databricks offers additional features like Delta Lake for transactional data handling and performance optimizations like Delta Auto Optimize.\n",
    "Understanding these internals helps in effectively designing and tuning Spark jobs for optimal performance and scalability in Databricks environments using PySpark."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks | Pyspark | Spark Architecture: Internals of Partition Creation Demystified",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
