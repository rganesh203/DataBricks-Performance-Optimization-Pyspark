{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "537ce1da-e965-4b1b-ab33-d6ef54af0ae5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimizing the performance of your Spark jobs in Databricks often involves managing the number of partitions in your DataFrame or RDD. Two key methods for this are repartition and coalesce. Both methods are used to control the number of partitions, but they have different use cases and performance implications. Here's a detailed comparison with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c677984-87b3-478e-9761-cd092fd03df5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Repartition\n",
    "\n",
    "repartition is used to increase or decrease the number of partitions in your DataFrame or RDD. It performs a full shuffle of the data, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "306b8380-2650-45bd-9268-aeedd2be915c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When to Use Repartition\n",
    "Increasing Partitions: Use repartition when you need to increase the number of partitions to distribute data more evenly across the cluster.\n",
    "Balancing Workload: Use it when the data is skewed and some partitions are significantly larger than others.\n",
    "Upstream of Expensive Operations: Use it before a costly operation like a join to ensure the data is evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63358e06-6fb4-4b47-bbdf-f94b9973a517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 8\nPartitions after repartition: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RepartitionExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(i,) for i in range(100)]\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "# Check the number of partitions\n",
    "print(\"Initial partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Repartition the DataFrame\n",
    "df_repartitioned = df.repartition(10)\n",
    "\n",
    "# Check the number of partitions after repartitioning\n",
    "print(\"Partitions after repartition:\", df_repartitioned.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a30930a-1526-4c13-8e53-82db95943702",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Coalesce\n",
    "\n",
    "coalesce is used to reduce the number of partitions. Unlike repartition, coalesce tries to minimize the amount of data movement by combining partitions. This makes it much more efficient than repartition for decreasing the number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1310d52c-6fa9-4200-b4de-9fe4ced564d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When to Use Coalesce\n",
    "Decreasing Partitions: Use coalesce when you need to reduce the number of partitions, typically after filtering a large DataFrame.\n",
    "Post-Filter Operations: Use it after a filter operation to avoid shuffling large amounts of data.\n",
    "Optimizing Small Writes: Use it before writing out small amounts of data to reduce the number of output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e885ead4-beb9-4122-b4cc-6a7d040ede30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 8\nPartitions after coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions\n",
    "print(\"Initial partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce the DataFrame\n",
    "df_coalesced = df.coalesce(2)\n",
    "\n",
    "# Check the number of partitions after coalescing\n",
    "print(\"Partitions after coalesce:\", df_coalesced.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b06d1d-fb93-4ae5-aa6d-e8d4e7b6c69e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Performance Considerations\n",
    "\n",
    "Shuffling: repartition involves a full shuffle of the data across the cluster, which is costly. coalesce, on the other hand, avoids a full shuffle, making it faster for reducing partitions.\n",
    "\n",
    "Resource Utilization: Use repartition to increase partitions for better parallelism and resource utilization. Use coalesce to reduce the number of partitions to optimize for tasks that donâ€™t require high parallelism, such as writing smaller datasets.\n",
    "\n",
    "Data Skew: Address data skew by using repartition to ensure an even distribution of data across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e11aa4-135d-4d46-89d6-6806431f1473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Practical Example in Databricks\n",
    "Assume you have a large dataset and you want to perform operations efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff003387-7ce7-46d7-919e-0e88be4ea66f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load a large dataset\n",
    "df = spark.read.parquet(\"path/to/large/dataset\")\n",
    "\n",
    "# Initial partitions\n",
    "initial_partitions = df.rdd.getNumPartitions()\n",
    "print(f\"Initial partitions: {initial_partitions}\")\n",
    "\n",
    "# Repartition for better parallelism\n",
    "df_repartitioned = df.repartition(100)\n",
    "print(f\"Partitions after repartition: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Perform some transformations\n",
    "df_transformed = df_repartitioned.filter(df[\"value\"] > 100)\n",
    "\n",
    "# Coalesce to reduce partitions before writing out\n",
    "df_coalesced = df_transformed.coalesce(10)\n",
    "print(f\"Partitions after coalesce: {df_coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Write the transformed data\n",
    "df_coalesced.write.parquet(\"path/to/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ed6357-cf93-4f9e-bcf3-5fd235257dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this example:\n",
    "\n",
    "repartition(100) increases the number of partitions for better parallelism.\n",
    "\n",
    "After filtering, coalesce(10) reduces the number of partitions to optimize the write operation.\n",
    "\n",
    "By understanding and leveraging repartition and coalesce appropriately, you can significantly optimize the performance of your Spark jobs in Databricks."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization | Repartition vs Coalesce",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
