{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce15ee1-2bea-4cca-8c44-cf366df5479a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the partitionBy option in Databricks can significantly enhance performance for various operations such as reading, writing, and querying large datasets. Partitioning helps distribute data evenly across different storage blocks, allowing Spark to parallelize operations more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2cb28e0-0f95-4e89-ac96-ea22f6268288",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Why Use partitionBy?\n",
    "Improved Query Performance: When data is partitioned, Spark can skip entire partitions that are not relevant to a query, reducing the amount of data read.\n",
    "Efficient Resource Utilization: By splitting data into smaller, manageable chunks, Spark can distribute tasks across multiple nodes, improving resource utilization.\n",
    "Faster Writes: Writing partitioned data can be faster as it organizes data into directory structures, reducing the I/O overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4264ab6f-61c1-4082-94f1-abcc98e56c29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example: Using partitionBy in Databricks\n",
    "Step 1: Create a Spark DataFrame\n",
    "Let's start by creating a sample DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951bc419-22d9-4ed4-93f9-a5de77fb5f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+\n| name|department|year|\n+-----+----------+----+\n|Alice|     Sales|2022|\n|  Bob| Marketing|2022|\n|Cathy|     Sales|2023|\n|David| Marketing|2023|\n|  Eve|        IT|2022|\n|Frank|        IT|2023|\n+-----+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionByExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"Sales\", 2022),\n",
    "    (\"Bob\", \"Marketing\", 2022),\n",
    "    (\"Cathy\", \"Sales\", 2023),\n",
    "    (\"David\", \"Marketing\", 2023),\n",
    "    (\"Eve\", \"IT\", 2022),\n",
    "    (\"Frank\", \"IT\", 2023)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"department\", \"year\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db6eacc-cbcd-4387-9d88-c7fc21554b86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2: Write Data with partitionBy\n",
    "Write the DataFrame to a Delta table, partitioning by the year column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3e41b2-30ad-434f-b9e0-b8b08be6b5d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to Delta table with partitioning\n",
    "output_path = \"/mnt/delta/employee_data\"\n",
    "df.write.format(\"delta\").partitionBy(\"year\").mode(\"overwrite\").save(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c09984bb-da18-4719-a12f-87c1d9f475d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3: Read Partitioned Data\n",
    "Read the partitioned Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98eadf1a-5405-4bc4-bfe1-3a4fa6b2cd92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read partitioned Delta table\n",
    "df_partitioned = spark.read.format(\"delta\").load(output_path)\n",
    "df_partitioned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cabde2ed-5e95-41b6-82b2-d65b6df9f03a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 4: Query Performance Optimization\n",
    "When querying the partitioned data, Spark can skip irrelevant partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0abae43-4e96-44c3-8491-db9d1c8409b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for the year 2022\n",
    "df_2022 = df_partitioned.filter(col(\"year\") == 2022)\n",
    "df_2022.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b121c30-30e6-40a4-920a-cbb95551aa4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since the data is partitioned by year, the filter operation will be efficient as Spark will only read the partitions for the year 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a47510e-83a5-4235-b4fc-7adea4deb1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example with a Larger Dataset\n",
    "For a larger dataset, let's consider a hypothetical scenario where we have sales data with columns: order_id, product_id, customer_id, order_date, and amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac7d62f-6efd-4596-babf-7a4df7fe6bac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 1: Create a Larger DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3285521c-3d03-47c0-8954-5645098c4778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a large dataset\n",
    "num_records = 1000000\n",
    "data = {\n",
    "    \"order_id\": np.arange(1, num_records + 1),\n",
    "    \"product_id\": np.random.randint(1, 1000, size=num_records),\n",
    "    \"customer_id\": np.random.randint(1, 5000, size=num_records),\n",
    "    \"order_date\": pd.date_range(start=\"2023-01-01\", periods=num_records, freq=\"T\"),\n",
    "    \"amount\": np.random.uniform(10.0, 1000.0, size=num_records)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "pdf = pd.DataFrame(data)\n",
    "large_df = spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5987012-8c64-446e-ab28-dd5905142fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2: Write Large Data with partitionBy\n",
    "Partition the large DataFrame by order_date (year and month):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110e0a28-bb6f-44e9-901d-4285f0e28761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write large DataFrame with partitioning by year and month\n",
    "large_df = large_df.withColumn(\"year\", year(col(\"order_date\")))\n",
    "large_df = large_df.withColumn(\"month\", month(col(\"order_date\")))\n",
    "\n",
    "output_path_large = \"/mnt/delta/large_sales_data\"\n",
    "large_df.write.format(\"delta\").partitionBy(\"year\", \"month\").mode(\"overwrite\").save(output_path_large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a2586e-eb0d-4b09-829e-f322819e1b19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3: Querying Partitioned Large Data\n",
    "When querying, Spark will efficiently scan only the relevant partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e70e007-0ed0-4d09-9329-95c36cfbcbb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read partitioned large Delta table\n",
    "large_df_partitioned = spark.read.format(\"delta\").load(output_path_large)\n",
    "\n",
    "# Filter data for January 2023\n",
    "df_jan_2023 = large_df_partitioned.filter((col(\"year\") == 2023) & (col(\"month\") == 1))\n",
    "df_jan_2023.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea1ec67-57cf-41a0-80bf-4b73822e176a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conclusion\n",
    "Using partitionBy in Databricks with Delta Lake can greatly enhance the performance of your data processing tasks by enabling efficient data partitioning. This allows Spark to read and write data more efficiently, improving overall query performance and resource utilization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization PartitionBy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
