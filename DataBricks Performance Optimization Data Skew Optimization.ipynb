{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f4f71d-cc1c-4f35-b739-00bb8dddc473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Optimizing performance in Databricks, particularly addressing data skew, is crucial for ensuring efficient execution of large-scale data processing tasks. Data skew occurs when the distribution of data is uneven across partitions, leading to some partitions having significantly more data than others. This can cause certain tasks to take much longer to complete, bottlenecking the entire job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90a21655-b682-4b7a-86ea-619c9831d229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Identifying Data Skew\n",
    "Before addressing data skew, it's essential to identify it. You can use the following techniques to spot data skew:\n",
    "\n",
    "Spark UI: Check the stages and tasks in the Spark UI for uneven task durations.\n",
    "Logs and Metrics: Look for imbalances in logs and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab82ac7e-b40a-40f3-984a-fbcfbc4e8f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example Scenario\n",
    "Suppose you have a DataFrame df with a column category that is highly skewed, meaning some categories appear much more frequently than others.\n",
    "\n",
    "Addressing Data Skew\n",
    "Here are some strategies to address data skew in Databricks:\n",
    "\n",
    "1. Salting\n",
    "Salting involves adding a random component to the key, thereby distributing the data more evenly across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed70048-ada9-48fb-9d8f-0cda02c736a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataSkewOptimization\").getOrCreate()\n",
    "\n",
    "# Example DataFrame\n",
    "data = [(\"A\", 1), (\"A\", 2), (\"B\", 1), (\"C\", 1), (\"A\", 3)]\n",
    "df = spark.createDataFrame(data, [\"category\", \"value\"])\n",
    "\n",
    "# Adding a salt column\n",
    "salted_df = df.withColumn(\"salt\", F.expr(\"floor(rand() * 10)\"))\n",
    "\n",
    "# Create a new key by combining category and salt\n",
    "salted_df = salted_df.withColumn(\"salted_category\", F.concat(F.col(\"category\"), F.col(\"salt\")))\n",
    "\n",
    "# Perform the join or aggregation on the salted key\n",
    "# Example: Group by the salted key\n",
    "result = salted_df.groupBy(\"salted_category\").agg(F.sum(\"value\").alias(\"total_value\"))\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b054cbeb-5b57-4857-83a0-aab6234ffffc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Broadcast Joins\n",
    "If one of the tables in a join is small enough to fit into memory, broadcast it to all nodes to avoid shuffling large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b999d507-d975-4273-a6b1-7fe36a3fe13d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Small DataFrame to be broadcasted\n",
    "small_df = spark.createDataFrame([(\"A\", \"info1\"), (\"B\", \"info2\")], [\"category\", \"info\"])\n",
    "\n",
    "# Large DataFrame\n",
    "large_df = spark.createDataFrame([(\"A\", 1), (\"A\", 2), (\"B\", 1), (\"C\", 1), (\"A\", 3)], [\"category\", \"value\"])\n",
    "\n",
    "# Perform a broadcast join\n",
    "joined_df = large_df.join(broadcast(small_df), \"category\")\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e40e2c-e536-4d7a-ad9c-2d4d346b079f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Repartitioning\n",
    "Repartitioning the DataFrame based on a different key or using a custom partitioner can help distribute data more evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca97112-1fa4-4a53-8f63-6815908a3c90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartitioning based on a different key\n",
    "repartitioned_df = df.repartition(10, \"category\")\n",
    "\n",
    "# Perform operations on the repartitioned DataFrame\n",
    "result = repartitioned_df.groupBy(\"category\").agg(F.sum(\"value\").alias(\"total_value\"))\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da562c1-12eb-445b-a7d2-c7131b31548e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Using Adaptive Query Execution (AQE)\n",
    "Databricks supports Adaptive Query Execution (AQE), which dynamically optimizes query plans based on runtime statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5377cba1-f218-4b44-84a0-50dd6f4b85e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Execute your queries as usual\n",
    "result = df.groupBy(\"category\").agg(F.sum(\"value\").alias(\"total_value\"))\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8d282b-d4f9-4549-8591-35bf29840d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Best Practices\n",
    "Monitor Spark UI: Regularly monitor the Spark UI for signs of skew and uneven task durations.\n",
    "Tune Cluster Resources: Adjust the number and size of executors and partitions to match your workload.\n",
    "Optimize Joins and Aggregations: Use broadcast joins and aggregation strategies to minimize shuffling.\n",
    "Regular Maintenance: Periodically review and refactor your data processing logic to address any performance bottlenecks.\n",
    "By implementing these strategies, you can effectively manage and optimize data skew in Databricks, leading to improved performance and more efficient data processing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Data Skew Optimization",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
