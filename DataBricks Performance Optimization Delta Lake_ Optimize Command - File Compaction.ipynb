{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65e1d11f-ae58-4812-8188-ff04b4511e7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The OPTIMIZE command in Delta Lake is used to compact small files into larger ones, which can significantly improve read performance by reducing the number of files read during queries. This is particularly useful in big data environments where data is frequently ingested in small batches, leading to a large number of small files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370de6d3-3ebc-4c25-b215-1bc007ba5215",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Benefits of File Compaction\n",
    "\n",
    "Improved Query Performance: By reducing the number of files, the optimizer can more efficiently scan and process data.\n",
    "\n",
    "Reduced File System Overhead: Fewer files reduce the overhead on the file system, improving overall system performance.\n",
    "\n",
    "Enhanced Data Skipping: Larger files allow better data skipping capabilities, which means faster query times for queries with selective filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a05cced-3e5c-41be-a9a4-0b03d84bcf7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How to Use the OPTIMIZE Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b56c5e2-aec7-417b-9e2e-ec1e738a52f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZE delta.`<path-to-delta-table>` [WHERE <predicate>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eddc220-6b3e-4f1e-bbae-226f796afc4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<path-to-delta-table>: The path to the Delta table you want to optimize.\n",
    "[WHERE <predicate>]: Optional. A condition to specify which partitions or files to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caaba32d-9be9-43ab-8032-31ad06bef55e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example Usage\n",
    "Let's walk through an example of how to use the OPTIMIZE command in Databricks.\n",
    "\n",
    "1. Create a Delta Table\n",
    "First, create a sample Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b0d2a5b-798f-4793-9e55-9d9226834332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeOptimizeExample\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\"), (4, \"David\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09cf593d-9e23-4d89-9d46-33ec5ebcde13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Simulate Small File Creation\n",
    "Let's simulate small file creation by appending data multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bf2fc9c-4a13-422a-870b-20821377883e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5, 20):\n",
    "    new_data = [(i, f\"User{i}\")]\n",
    "    new_df = spark.createDataFrame(new_data, [\"id\", \"name\"])\n",
    "    new_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta-table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29afff5f-6bd6-47e4-acc1-e1e1d88ff2e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Optimize the Delta Table\n",
    "Now, run the OPTIMIZE command to compact the small files into larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce430c07-67a1-4df1-8b79-8769dd141bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE delta.`/tmp/delta-table`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01cbc01a-b978-415c-8523-70d229f51a23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also run the OPTIMIZE command from a Databricks notebook cell with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b7b2cc7-abb6-42d4-8f96-50c2ab7923e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE delta.`/tmp/delta-table`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "168de9f4-6983-480d-82e3-3011fea26068",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Optimize with Predicate\n",
    "If your Delta table is partitioned, you can specify a predicate to optimize only specific partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a7eb67f-7eaa-43bb-a2b9-11110a2c302d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE delta.`/tmp/delta-table` WHERE id > 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4ae511-a098-4821-93c2-ec2476745296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c45eaa-0148-493b-89c9-fcc275ca97ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE delta.`/tmp/delta-table` WHERE id > 10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd200c3-099f-410f-a7eb-ca6952011ab5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Monitoring and Maintenance\n",
    "After running the OPTIMIZE command, you can check the Delta table's performance by:\n",
    "\n",
    "Query Execution Plans: Examine the query execution plans to ensure that the number of files read has decreased.\n",
    "Delta Table History: Check the Delta table history to see the optimization operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a31e8d-4e36-4c3a-a60a-72be0786ac6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
    "deltaTable.history().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1d36d35-e824-4c1f-8f31-668f00b64055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vacuum Command: To ensure that the old files are properly cleaned up, use the VACUUM command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4664e73b-3fd8-4941-8d6b-abdf88bb08d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "VACUUM delta.`/tmp/delta-table` RETAIN 168 HOURS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c752e5-270c-45dc-bf93-b4a755ba5307",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb948485-de60-41ba-8a3d-d557853f7236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"VACUUM delta.`/tmp/delta-table` RETAIN 168 HOURS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4cf0f4e-098a-4e7a-a166-270aa7a0e6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conclusion:\n",
    "\n",
    "The OPTIMIZE command is a powerful tool in Delta Lake for improving the performance of your Delta tables by compacting small files into larger ones. This results in faster query performance and more efficient storage management. Regularly optimizing your Delta tables can help maintain optimal performance in your Databricks environment."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Delta Lake: Optimize Command - File Compaction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
