{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc63123e-5ed1-434e-97e7-429d8aac0c43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In Databricks, optimizing performance often involves leveraging broadcast variables to efficiently handle joins between large and small DataFrames. Broadcast variables help reduce the amount of data shuffled across the cluster, thereby improving query performance. Here's how you can optimize performance using broadcast variables with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4d1c56-93bf-46d0-a909-b809469c7ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Understanding Broadcast Variables\n",
    "Broadcast variables in Spark allow you to efficiently distribute a large read-only variable to all the nodes in a Spark cluster. They are particularly useful when you have a small DataFrame or dataset that can fit into memory across all nodes, which is then broadcasted rather than shuffled across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d411cce-5fd1-4139-bdaa-dcf16e32527e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example Scenario\n",
    "\n",
    "Let's consider a scenario where you have two DataFrames:\n",
    "\n",
    "Large DataFrame (large_df): Contains a large amount of data.\n",
    "Small DataFrame (small_df): Contains a small amount of data that can fit into memory across all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2665ae71-5e3e-49e5-ac15-c9aa240624b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimization Using Broadcast Variables\n",
    "Suppose you want to join these two DataFrames based on a common key, and small_df is significantly smaller compared to large_df. Here's how you can optimize the join operation using broadcast variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b9c7d10-1000-4a78-b433-6850dc121fed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Broadcast the Small DataFrame:\n",
    "\n",
    "You can explicitly broadcast small_df before performing the join operation. This ensures that small_df is efficiently distributed to all the worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24dc3490-3982-4e93-8ae4-fdbdb8d69b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Broadcast Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assume you have large_df and small_df already defined\n",
    "\n",
    "# Broadcast small_df\n",
    "broadcast_small_df = broadcast(small_df)\n",
    "\n",
    "# Perform the join with broadcast hint\n",
    "joined_df = large_df.join(broadcast_small_df, on='common_key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e1750d0-39c8-459f-ba2e-8e79324bc8a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the example above:\n",
    "\n",
    "broadcast(small_df) creates a broadcast variable from small_df.\n",
    "joined_df performs the join operation between large_df and broadcast_small_df using the common key (common_key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7ea8e14-cef7-44bc-9b69-4da929626979",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Execution Plan:\n",
    "\n",
    "Spark's query optimizer recognizes the broadcast hint (broadcast(small_df)) and optimizes the join operation. It will use broadcast join strategy, which avoids shuffling small_df across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78874592-bc99-4b8a-9d38-c877c5b559c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Benefits:\n",
    "\n",
    "By broadcasting small_df, you avoid the overhead of shuffling the entire small_df dataset across the network.\n",
    "This reduces network traffic and improves the overall performance of the join operation, especially when small_df is significantly smaller than large_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8cf4b98-6060-40cb-971e-cf871380719f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Considerations\n",
    "\n",
    "Size: Ensure that small_df is indeed small enough to be broadcasted. Spark has a default broadcast threshold of 10 MB, but you can adjust this threshold if needed (spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", <size_in_bytes>)).\n",
    "\n",
    "Memory: Broadcasting too large datasets can lead to out-of-memory errors. Monitor memory usage when working with broadcast variables.\n",
    "\n",
    "Query Performance: Test and benchmark different join strategies (broadcast vs. shuffle) to determine the optimal approach for your specific workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf91d7ac-b70a-44d1-a100-5d13a0187201",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By leveraging broadcast variables in Databricks, you can significantly optimize performance for join operations involving large and small DataFrames, improving efficiency and reducing execution time."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Broadcast Variable|",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
