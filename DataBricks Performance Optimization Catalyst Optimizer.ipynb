{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f25bc869-8a84-405f-bbac-0fc395d5e26d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Catalyst Optimizer is a key component of Apache Spark SQL that optimizes queries for execution. In Databricks, this optimizer plays a crucial role in enhancing the performance of your Spark SQL queries. Here's a breakdown of how the Catalyst Optimizer works, along with an example to illustrate its impact on performance optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f30ea2-3cf4-479a-8717-52f7e3dda121",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What is the Catalyst Optimizer?\n",
    "The Catalyst Optimizer is a query optimization framework built into Spark SQL. It consists of several phases:\n",
    "\n",
    "Analysis: Parses and resolves the logical plan, checking for syntax errors and resolving references to columns and tables.\n",
    "Logical Optimization: Applies rule-based optimizations to the logical plan, such as predicate pushdown, constant folding, and projection pruning.\n",
    "Physical Planning: Converts the optimized logical plan into one or more physical plans and selects the best physical plan based on cost.\n",
    "Code Generation: Uses a code generation technique called \"whole-stage code generation\" to compile parts of the query into Java bytecode, improving execution efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9cf75f-02c5-4c91-816f-0de29da7d42c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example: Using Catalyst Optimizer in Databricks\n",
    "Let's walk through an example to see how the Catalyst Optimizer optimizes a query in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe1c3d19-fcd1-46b4-93ec-dbebf4cdd7fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 1: Setup\n",
    "First, we'll create a sample DataFrame in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7977713c-787b-403b-9a17-fc54c6f523d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, \"Alice\", 34), (2, \"Bob\", 45), (3, \"Cathy\", 29)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"people\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b20f42-375c-4d05-8e9c-3108a9978bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2: Writing a Query\n",
    "Let's write a simple SQL query to select data from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d86c5f64-d413-4d50-abf9-54eef17d849b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT id, name, age \n",
    "FROM people \n",
    "WHERE age > 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cc82215-b252-4261-9481-1669249c33c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3: Understanding Catalyst Optimizer\n",
    "When this query is executed, the Catalyst Optimizer goes through the following stages:\n",
    "\n",
    "Analysis:\n",
    "\n",
    "Parses the SQL query.\n",
    "Resolves the column references (id, name, age) and the table reference (people).\n",
    "Logical Optimization:\n",
    "\n",
    "Applies predicate pushdown: The filter condition (age > 30) is pushed down to reduce the number of rows processed in subsequent operations.\n",
    "Projection pruning: Only the required columns (id, name, age) are selected.\n",
    "Physical Planning:\n",
    "\n",
    "Determines the most efficient physical plan to execute the query. For instance, it may choose a simple scan operation on the people DataFrame.\n",
    "Code Generation:\n",
    "\n",
    "Generates Java bytecode for parts of the query execution, improving execution speed by reducing interpretation overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5266525e-8cd9-4732-a9d6-ff059c92af87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 4: Viewing the Optimized Plan\n",
    "You can view the optimized logical and physical plans using the explain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f26166c-9cfa-446c-b89c-bf8dac126f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = spark.sql(\"SELECT id, name, age FROM people WHERE age > 30\")\n",
    "df_filtered.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "054a419e-25ad-45c0-9fff-0ff5caaf319c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This will output the query plan, including the logical and physical plans. Here's a simplified version of what you might see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993f11b0-09c2-4533-a548-31e322832a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "== Parsed Logical Plan ==\n",
    "'Project [id#0, name#1, age#2]\n",
    "+- 'Filter (age#2 > 30)\n",
    "   +- 'UnresolvedRelation [people]\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "id: int, name: string, age: int\n",
    "Project [id#0, name#1, age#2]\n",
    "+- Filter (age#2 > 30)\n",
    "   +- SubqueryAlias people\n",
    "      +- LogicalRDD [id#0, name#1, age#2], false\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "Project [id#0, name#1, age#2]\n",
    "+- Filter (age#2 > 30)\n",
    "   +- LogicalRDD [id#0, name#1, age#2], false\n",
    "\n",
    "== Physical Plan ==\n",
    "*(1) Project [id#0, name#1, age#2]\n",
    "+- *(1) Filter (age#2 > 30)\n",
    "   +- *(1) Scan ExistingRDD [id#0, name#1, age#2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80412e3b-96a2-423e-a7e7-bba1928b61c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimizations Observed\n",
    "Predicate Pushdown: The filter (age > 30) is applied as early as possible, reducing the number of rows processed.\n",
    "Projection Pruning: Only the columns id, name, and age are selected, reducing the amount of data shuffled and processed.\n",
    "Efficient Physical Plan: The physical plan shows a direct scan operation with a filter and projection, ensuring minimal overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64abaa8b-0e2b-4cbf-bf05-11a0cee577d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conclusion\n",
    "The Catalyst Optimizer in Spark SQL on Databricks performs several crucial optimizations to enhance query performance. By understanding and leveraging these optimizations, you can write more efficient queries and achieve better performance in your data processing workflows.\n",
    "\n",
    "For more advanced use cases, consider:\n",
    "\n",
    "Indexing: Using Delta Lake indexing features for faster lookups.\n",
    "Materialized Views: Precomputing and storing common query results for faster access.\n",
    "Adaptive Query Execution (AQE): Dynamically optimizing query execution based on runtime statistics."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Catalyst Optimizer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
