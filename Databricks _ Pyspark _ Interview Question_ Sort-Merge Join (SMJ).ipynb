{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d891e6-e649-4488-a557-7a94c232edae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sort-Merge Join (SMJ) is a common join algorithm used in Apache Spark and PySpark for joining large datasets efficiently. In Databricks, leveraging SMJ can significantly optimize the performance of join operations, particularly when dealing with large, sorted datasets. Here's an overview of SMJ, its mechanics, and practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0b3f06-7aa7-4670-9111-1d8743a37689",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Sort-Merge Join Overview\n",
    "Sort-Merge Join is efficient when joining large datasets because it:\n",
    "\n",
    "Sorts Both Datasets: Each dataset is sorted by the join key.\n",
    "Merges the Sorted Datasets: After sorting, the datasets are merged by matching join keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "049afd1f-5ad5-4f93-b026-358e3090bff9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Mechanics of Sort-Merge Join\n",
    "Sorting Phase:\n",
    "Each dataset is sorted by the join key. This sorting ensures that matching keys can be merged efficiently.\n",
    "Merging Phase:\n",
    "The sorted datasets are scanned sequentially. Since both datasets are sorted, matching keys are found quickly by comparing the current elements in each dataset and advancing the pointers accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdbefb4-64e2-4358-907c-8fc5e6bfbaca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Performance Considerations\n",
    "Memory Usage: SMJ can be memory intensive as sorting large datasets may require substantial memory.\n",
    "\n",
    "Shuffle Operations: Sorting involves shuffle operations, which can be expensive. Proper partitioning and bucketing can help mitigate the overhead.\n",
    "\n",
    "Spill to Disk: If the data is too large to fit in memory, Spark may spill to disk, affecting performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c8b7c6-6ad4-4f8a-93b1-70b4fd5de8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Practical Example of Sort-Merge Join in PySpark\n",
    "Hereâ€™s how you can perform a Sort-Merge Join in PySpark within a Databricks environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323d3b6f-d77a-4436-850c-ee55b9bc69d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>department</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>HR</td></tr><tr><td>3</td><td>Cathy</td><td>Finance</td></tr><tr><td>2</td><td>Bob</td><td>Engineering</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "HR"
        ],
        [
         3,
         "Cathy",
         "Finance"
        ],
        [
         2,
         "Bob",
         "Engineering"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sort-Merge Join Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create example DataFrames\n",
    "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")]\n",
    "data2 = [(1, \"HR\"), (2, \"Engineering\"), (3, \"Finance\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"department\"])\n",
    "\n",
    "# Repartition to simulate larger datasets and to ensure join uses SMJ\n",
    "df1 = df1.repartition(4, col(\"id\"))\n",
    "df2 = df2.repartition(4, col(\"id\"))\n",
    "\n",
    "# Perform Sort-Merge Join\n",
    "joined_df = df1.join(df2, \"id\")\n",
    "\n",
    "# Show result\n",
    "joined_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739c3ad7-1f0e-4bd8-8108-1bc32023f9fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Explanation\n",
    "Initialize Spark Session: Start the Spark session with necessary configurations.\n",
    "\n",
    "Create Example DataFrames: Define two sample DataFrames, df1 and df2, with some data.\n",
    "\n",
    "Repartition DataFrames: Use repartition(4, col(\"id\")) to ensure the data is partitioned by the join key. This step helps in using Sort-Merge Join by ensuring the data is distributed across the cluster based on the join key.\n",
    "\n",
    "Perform Sort-Merge Join: Use the join method to join the DataFrames on the id column. Spark will use Sort-Merge Join because the data is partitioned by the join key.\n",
    "\n",
    "Show Result: Display the result of the join operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e93058-4a15-47b8-a7c7-9cb5469a38bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Best Practices for Using Sort-Merge Join\n",
    "Ensure Adequate Memory: Ensure that your cluster has sufficient memory to handle the sorting phase of the join.\n",
    "Proper Partitioning: Partition your datasets by the join key to minimize shuffle operations.\n",
    "Combine with Bucketing: For even better performance, consider bucketing your datasets on the join key. This reduces the shuffle during the join phase and can significantly speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e632c2-3670-47b5-94fd-c346f45b3b34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Conclusion\n",
    "Sort-Merge Join is a powerful technique for optimizing join operations in Spark, especially for large datasets. By understanding its mechanics and best practices, you can effectively leverage SMJ to improve the performance of your data processing workflows in Databricks using PySpark. This is a common topic in interviews, reflecting your understanding of distributed data processing and performance optimization techniques."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks | Pyspark | Interview Question: Sort-Merge Join (SMJ)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
