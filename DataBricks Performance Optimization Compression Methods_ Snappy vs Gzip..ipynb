{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98137fc-45d8-41e2-8215-f024c79de885",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When optimizing performance in Databricks, choosing the right compression method is essential. Snappy and Gzip are two popular compression methods, each with its own advantages and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339b6a8b-eb13-4e34-b284-af399dd884ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Snappy vs. Gzip\n",
    "###Snappy:\n",
    "\n",
    "Compression Speed: Fast compression and decompression speed.\n",
    "Compression Ratio: Moderate compression ratio.\n",
    "Use Case: Ideal for scenarios where speed is more critical than compression ratio, such as real-time analytics and iterative machine learning algorithms.\n",
    "\n",
    "###Gzip:\n",
    "\n",
    "Compression Speed: Slower compression and decompression speed compared to Snappy.\n",
    "Compression Ratio: Higher compression ratio, resulting in smaller file sizes.\n",
    "Use Case: Suitable for scenarios where storage efficiency is more important, such as archiving and long-term storage of large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3beb8804-cbcd-49a4-9a04-939fe797d562",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example: Comparing Snappy and Gzip in Databricks\n",
    "Let's go through an example to compare the performance of Snappy and Gzip in Databricks using PySpark.\n",
    "\n",
    "Setup\n",
    "Assume we have a large DataFrame df that we want to write to a Delta Lake table with different compression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d6ce305-66ed-4248-a679-ccc921e95a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CompressionComparison\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(i, f\"value_{i}\") for i in range(1000000)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "# Write DataFrame with Snappy compression\n",
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .save(\"/tmp/delta_snappy\")\n",
    "\n",
    "# Write DataFrame with Gzip compression\n",
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"gzip\") \\\n",
    "  .save(\"/tmp/delta_gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f64a03-1d4c-4a69-a307-28d026c59961",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Performance Measurement\n",
    "We can measure the write time, read time, and storage size for both compression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153fe27f-4417-491f-9296-619845658ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure write time\n",
    "def measure_write_time(df, format, compression, path):\n",
    "    start_time = time.time()\n",
    "    df.write \\\n",
    "      .format(format) \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"compression\", compression) \\\n",
    "      .save(path)\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Function to measure read time\n",
    "def measure_read_time(path):\n",
    "    start_time = time.time()\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    df.count()  # Trigger action to measure read time\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Measure write time for Snappy\n",
    "snappy_write_time = measure_write_time(df, \"delta\", \"snappy\", \"/tmp/delta_snappy\")\n",
    "# Measure read time for Snappy\n",
    "snappy_read_time = measure_read_time(\"/tmp/delta_snappy\")\n",
    "\n",
    "# Measure write time for Gzip\n",
    "gzip_write_time = measure_write_time(df, \"delta\", \"gzip\", \"/tmp/delta_gzip\")\n",
    "# Measure read time for Gzip\n",
    "gzip_read_time = measure_read_time(\"/tmp/delta_gzip\")\n",
    "\n",
    "# Get file sizes\n",
    "snappy_size = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "    .getContentSummary(spark._jvm.org.apache.hadoop.fs.Path(\"/tmp/delta_snappy\")).getLength()\n",
    "\n",
    "gzip_size = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "    .getContentSummary(spark._jvm.org.apache.hadoop.fs.Path(\"/tmp/delta_gzip\")).getLength()\n",
    "\n",
    "# Print results\n",
    "print(f\"Snappy - Write time: {snappy_write_time} seconds, Read time: {snappy_read_time} seconds, Size: {snappy_size} bytes\")\n",
    "print(f\"Gzip - Write time: {gzip_write_time} seconds, Read time: {gzip_read_time} seconds, Size: {gzip_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171e8eab-997b-4cea-bfa5-17d03dba79c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Results Interpretation\n",
    "After running the above code, you should see output indicating the write time, read time, and storage size for both Snappy and Gzip compressed files.\n",
    "\n",
    "Choosing the Right Compression Method\n",
    "Snappy: Choose Snappy if your workload requires fast read and write operations and the slightly larger file size is acceptable.\n",
    "Gzip: Choose Gzip if you need better storage efficiency and can tolerate slower read and write speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf143bd-b6da-4701-9181-7402ea0a2600",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Best Practices\n",
    "Benchmarking: Always benchmark with your specific dataset and workload to make an informed decision.\n",
    "Testing in Production: Test in a production-like environment to ensure that the chosen compression method meets your performance and storage requirements.\n",
    "Hybrid Approach: In some cases, using a combination of compression methods for different parts of your pipeline can offer the best balance of performance and storage efficiency.\n",
    "By carefully selecting the appropriate compression method, you can optimize both the performance and cost of your Databricks workloads."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Performance Optimization Compression Methods: Snappy vs Gzip.",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
