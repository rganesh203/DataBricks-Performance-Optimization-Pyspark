{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d0f002e-ef3c-471e-ab7c-7c452f9b04bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimizing shuffle partitions in Databricks is crucial for enhancing performance, especially when dealing with large datasets and operations that involve shuffling data across nodes (such as joins, groupBy, or aggregations). Hereâ€™s how to optimize shuffle partitions with examples:\n",
    "\n",
    "Understanding Shuffle Partitions\n",
    "Shuffle partitions determine how the data is distributed across the cluster during shuffle operations. The default number of shuffle partitions in Spark is often set to 200, which may not be optimal for your specific workload. Too few partitions can lead to large partitions that cause memory issues, while too many can lead to overhead from managing too many small partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89984c09-238b-4c15-8cdc-7df9cf8fef38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Steps to Optimize Shuffle Partitions\n",
    "Check the Default Number of Shuffle Partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386c0937-8bc3-4539-9901-11b79c68092b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35683b8a-2219-4c0f-89c3-dae81e395013",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Set the Number of Shuffle Partitions:\n",
    "Adjust the number of shuffle partitions based on the size of your data and the cluster configuration. A general rule of thumb is to aim for partition sizes between 128 MB to 1 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc326ac-0962-4b9d-a561-f56f6076ecb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_number_of_partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051b7452-6de5-483e-a5be-6410495e3654",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example\n",
    "Let's consider a practical example where we have a large DataFrame, and we want to optimize the shuffle partitions for a join operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d738174-820f-4c11-b0c0-f227ddb86538",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step-by-Step Example\n",
    "Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd9c86c-0aa9-4af5-90f8-1a6f0aa3cd27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/large_dataset1.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/large_dataset2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cdfd24c-4ffd-4b7c-83a8-7dedabccff0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Check Default Shuffle Partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffee7aac-8859-41d9-ae0e-baa46796826b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "default_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Default shuffle partitions: {default_partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5172a1a-80e9-4515-ba14-8cdd330ec3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Set Optimal Shuffle Partitions:\n",
    "Based on your data size and cluster resources, set an optimal number of shuffle partitions. For example, if you have a large dataset, you might want to increase the number of partitions to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a18138-7608-4fc0-8df2-4302f0697538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimal_partitions = 500\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02de4b1b-93bf-422d-b862-4a03b6d72c64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Perform Join Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b34c8a2-0e94-46d6-a433-c93d51443e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60dd426b-e7f8-4595-ae77-c78e8c686d66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Action to Trigger Shuffle:\n",
    "Perform an action to trigger the shuffle, such as counting the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a956545f-47e5-4311-946e-82d9e2e9c86b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count = joined_df.count()\n",
    "print(f\"Row count: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fd03cd-fefa-464b-a50f-36cc091c384c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Monitoring and Tuning:\n",
    "Monitor the performance using Spark UI to ensure that the number of partitions is optimal. Look for signs of imbalance such as skewed partitions or excessive spill to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2361cbe-8ef4-476b-9c1b-09fed987fa76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### View Spark UI by navigating to the Spark application URL provided by Databricks.\n",
    "Best Practices\n",
    "Profiling Your Data: Understand your data size and distribution to set a suitable number of partitions.\n",
    "Iterative Tuning: Start with an estimated number of partitions and iteratively tune based on performance metrics.\n",
    "Avoid Hardcoding: Instead of hardcoding the number of partitions, consider dynamically setting it based on data size if your data volume varies significantly.\n",
    "Monitoring: Regularly monitor the Spark UI to check for performance bottlenecks related to shuffling.\n",
    "By carefully setting and tuning the number of shuffle partitions, you can significantly improve the performance of your Spark jobs in Databricks, ensuring efficient resource utilization and faster job completion."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Optimization Shuffle Partition",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
